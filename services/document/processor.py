"""
ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤ (ê°„ì†Œí™”)
í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¡œë“œ, ì²­í‚¹, ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from core.config import get_settings
from core.logging.config import get_logger

logger = get_logger(__name__)


class DocumentProcessor:
    """ê°„ì†Œí™”ëœ ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.settings = get_settings()
        self.processed_chunks = []

        # ì²­í‚¹ ì„¤ì •
        self.chunk_size = self.settings.CHUNK_SIZE
        self.chunk_overlap = self.settings.CHUNK_OVERLAP

        # ì§€ì› íŒŒì¼ í˜•ì‹ (í…ìŠ¤íŠ¸ë§Œ)
        self.supported_extensions = {".txt", ".md"}

    async def process_documents(self, data_path: Optional[str] = None) -> bool:
        """ë¬¸ì„œ ì²˜ë¦¬"""
        try:
            source_path = data_path or self.settings.DATA_PATH
            logger.info(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {source_path}")

            if not os.path.exists(source_path):
                logger.warning(f"âš ï¸ ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_path}")
                return await self._create_sample_documents()

            # ë¬¸ì„œ íŒŒì¼ ìˆ˜ì§‘
            document_files = self._collect_document_files(source_path)

            if not document_files:
                logger.warning("âš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                return await self._create_sample_documents()

            # ë¬¸ì„œ ì²˜ë¦¬
            self.processed_chunks = []
            total_processed = 0

            for file_path in document_files:
                try:
                    chunks = await self._process_single_file(file_path)
                    self.processed_chunks.extend(chunks)
                    total_processed += len(chunks)
                    logger.debug(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path} ({len(chunks)}ê°œ ì²­í¬)")

                except Exception as e:
                    logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {str(e)}")
                    continue

            logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {len(document_files)}ê°œ íŒŒì¼, {total_processed}ê°œ ì²­í¬")
            return True

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return False

    def _collect_document_files(self, data_path: str) -> List[str]:
        """ë¬¸ì„œ íŒŒì¼ ìˆ˜ì§‘ (í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ)"""
        try:
            document_files = []
            data_dir = Path(data_path)

            if data_dir.is_file():
                # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°
                if data_dir.suffix.lower() in self.supported_extensions:
                    document_files.append(str(data_dir))
            else:
                # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
                for file_path in data_dir.rglob("*"):
                    if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                        document_files.append(str(file_path))

            logger.debug(f"ğŸ“ ìˆ˜ì§‘ëœ ë¬¸ì„œ íŒŒì¼: {len(document_files)}ê°œ")
            return document_files

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ íŒŒì¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _process_single_file(self, file_path: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ê°„ì†Œí™”)"""
        try:
            file_path_obj = Path(file_path)

            logger.debug(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_path}")

            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            content = await self._read_text_file(file_path)

            if not content:
                logger.warning(f"âš ï¸ ë¹ˆ íŒŒì¼: {file_path}")
                return []

            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "file_path": file_path,
                "file_name": file_path_obj.name,
                "processed_at": datetime.now().isoformat(),
            }

            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self._create_chunks(content, metadata)

            return chunks

        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {str(e)}")
            return []

    async def _read_text_file(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°"""
        try:
            # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸°
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            logger.debug(f"âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ: {file_path}")
            return content

        except UnicodeDecodeError:
            # CP949 ì¸ì½”ë”©ìœ¼ë¡œ ì¬ì‹œë„
            try:
                with open(file_path, "r", encoding="cp949") as f:
                    content = f.read()
                logger.debug(f"âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ (CP949): {file_path}")
                return content
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {str(e)}")
                return ""

        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {str(e)}")
            return ""

    def _create_chunks(self, content: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ì²­í‚¹ (ê°„ì†Œí™”)"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_content = self._preprocess_text(content)

            if not cleaned_content or len(cleaned_content) < 50:
                logger.warning("âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ì²­í‚¹í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return []

            # ì²­í‚¹ ìˆ˜í–‰
            chunks = []
            text_length = len(cleaned_content)

            # ë¬¸ì„œê°€ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
            if text_length <= self.chunk_size:
                chunks.append(
                    {
                        "content": cleaned_content,
                        "source": metadata.get("file_name", "unknown"),
                        "metadata": {**metadata, "chunk_id": 0, "total_chunks": 1},
                    }
                )
            else:
                # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì²­í‚¹
                chunk_id = 0
                start = 0

                while start < text_length:
                    end = min(start + self.chunk_size, text_length)

                    # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
                    if end < text_length:
                        for i in range(end, max(start + self.chunk_size // 2, start + 100), -1):
                            if cleaned_content[i - 1] in ".!?\n":
                                end = i
                                break

                    chunk_content = cleaned_content[start:end].strip()

                    if chunk_content and len(chunk_content) > 20:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                        chunks.append(
                            {
                                "content": chunk_content,
                                "source": metadata.get("file_name", "unknown"),
                                "metadata": {**metadata, "chunk_id": chunk_id, "chunk_length": len(chunk_content)},
                            }
                        )
                        chunk_id += 1

                    # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ì˜¤ë²„ë© ê³ ë ¤)
                    start = end - self.chunk_overlap

                    if start >= text_length - self.chunk_overlap:
                        break

                # ì´ ì²­í¬ ìˆ˜ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                for chunk in chunks:
                    chunk["metadata"]["total_chunks"] = len(chunks)

            logger.debug(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            return chunks

        except Exception as e:
            logger.error(f"âŒ ì²­í‚¹ ì‹¤íŒ¨: {str(e)}")
            return []

    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê°„ì†Œí™”)"""
        try:
            # ê¸°ë³¸ ì •ë¦¬
            text = text.strip()

            # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
            text = re.sub(r"\s+", " ", text)

            # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ
            text = re.sub(r"\n{3,}", "\n\n", text)

            return text.strip()

        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return text

    async def _create_sample_documents(self) -> bool:
        """ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± (ê°„ì†Œí™”)"""
        try:
            logger.info("ğŸ“ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± ì¤‘...")

            sample_documents = [
                {
                    "content": """ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•ì— ë”°ë¥¸ ë³´ì¦ê¸ˆ ë°˜í™˜ ì ˆì°¨

1. ë‚´ìš©ì¦ëª… ë°œì†¡: ì„ëŒ€ì¸ì—ê²Œ ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìš”êµ¬í•˜ëŠ” ë‚´ìš©ì¦ëª…ì„ ë°œì†¡í•©ë‹ˆë‹¤.
2. ì„ì°¨ê¶Œë“±ê¸°ëª…ë ¹ ì‹ ì²­: ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•ì— ë”°ë¼ ì„ì°¨ê¶Œë“±ê¸°ëª…ë ¹ì„ ì‹ ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ì†Œì•¡ì‚¬ê±´ì‹¬íŒ ë˜ëŠ” ë¯¼ì‚¬ì†Œì†¡: ë³´ì¦ê¸ˆ ë°˜í™˜ ì†Œì†¡ì„ ì œê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. ê°•ì œì§‘í–‰: ìŠ¹ì†Œ íŒê²°ì„ ë°›ì€ í›„ ê°•ì œì§‘í–‰ì„ í†µí•´ ë³´ì¦ê¸ˆì„ íšŒìˆ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬ì˜ ì „ì„¸ë³´ì¦ê¸ˆë°˜í™˜ë³´ì¦ ë“±ì„ í†µí•´ ë³´ì¦ê¸ˆì„ ë³´í˜¸ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.""",
                    "source": "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•",
                    "metadata": {"category": "ë¶€ë™ì‚°", "type": "ë²•ë¥ ì¡°í•­"},
                },
                {
                    "content": """ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¥¸ ë¶€ë‹¹í•´ê³  êµ¬ì œ ì ˆì°¨

1. ë…¸ë™ìœ„ì›íšŒ ì‹ ì²­: í•´ê³ ì¼ë¡œë¶€í„° 3ê°œì›” ì´ë‚´ì— ë…¸ë™ìœ„ì›íšŒì— ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì„ í•´ì•¼ í•©ë‹ˆë‹¤.
2. ì¡°ì‚¬ ë° ì‹¬ë¬¸: ë…¸ë™ìœ„ì›íšŒì—ì„œ ì‚¬ì‹¤ê´€ê³„ë¥¼ ì¡°ì‚¬í•˜ê³  ë‹¹ì‚¬ìë¥¼ ì‹¬ë¬¸í•©ë‹ˆë‹¤.
3. êµ¬ì œëª…ë ¹: ë¶€ë‹¹í•´ê³ ê°€ ì¸ì •ë˜ë©´ ì›ì§ë³µì§ ë° ì„ê¸ˆìƒë‹¹ì•¡ ì§€ê¸‰ ëª…ë ¹ì´ ë‚´ë ¤ì§‘ë‹ˆë‹¤.
4. ì´í–‰ê°•ì œê¸ˆ: êµ¬ì œëª…ë ¹ì„ ì´í–‰í•˜ì§€ ì•Šìœ¼ë©´ ì´í–‰ê°•ì œê¸ˆì´ ë¶€ê³¼ë©ë‹ˆë‹¤.

í•´ê³ ëŠ” ì •ë‹¹í•œ ì´ìœ ê°€ ìˆì–´ì•¼ í•˜ë©°, í•´ê³ ì ˆì°¨ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.""",
                    "source": "ê·¼ë¡œê¸°ì¤€ë²•",
                    "metadata": {"category": "ê·¼ë¡œ", "type": "ë²•ë¥ ì¡°í•­"},
                },
            ]

            self.processed_chunks = sample_documents
            logger.info(f"âœ… ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ: {len(sample_documents)}ê°œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    def get_processed_chunks(self) -> List[Dict[str, Any]]:
        """ì²˜ë¦¬ëœ ë¬¸ì„œ ì²­í¬ ë°˜í™˜"""
        return self.processed_chunks

    async def get_processing_statistics(self) -> Dict[str, Any]:
        """ë¬¸ì„œ ì²˜ë¦¬ í†µê³„"""
        try:
            stats = {
                "total_chunks": len(self.processed_chunks),
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "supported_extensions": list(self.supported_extensions),
            }

            if self.processed_chunks:
                # ì†ŒìŠ¤ë³„ í†µê³„
                sources = {}
                categories = {}
                total_content_length = 0

                for chunk in self.processed_chunks:
                    source = chunk.get("source", "unknown")
                    category = chunk.get("metadata", {}).get("category", "unknown")
                    content_length = len(chunk.get("content", ""))

                    sources[source] = sources.get(source, 0) + 1
                    categories[category] = categories.get(category, 0) + 1
                    total_content_length += content_length

                stats.update(
                    {
                        "sources": sources,
                        "categories": categories,
                        "avg_chunk_length": total_content_length / len(self.processed_chunks),
                        "total_content_length": total_content_length,
                    }
                )

            return stats

        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
