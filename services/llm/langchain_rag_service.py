"""
ÏôÑÏ†ÑÌïú LangChain Í∏∞Î∞ò RAG ÏÑúÎπÑÏä§ (Memory ÌÜµÌï©)
ÏßàÎ¨∏ Î∂ÑÎ•ò ‚Üí Î¨∏ÏÑú Í≤ÄÏÉâ ‚Üí ÎãµÎ≥Ä ÏÉùÏÑ±ÏùÑ ÌïòÎÇòÏùò Ï≤¥Ïù∏ÏúºÎ°ú Ïó∞Í≤∞ÌïòÎ©∞,
LangChain MemoryÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÎåÄÌôî Îß•ÎùΩÏùÑ ÏûêÎèô Í¥ÄÎ¶¨Ìï©ÎãàÎã§.
"""

import os
from typing import List, Dict, Any, Optional
import json
import asyncio

# from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser, BaseOutputParser
from langchain_community.callbacks import get_openai_callback
from langchain.memory import ConversationBufferWindowMemory

from core.config import get_settings
from core.logging.config import get_logger
from infrastructure.database.vector_store import VectorStore
from services.search.hybrid_search import HybridSearchService
from services.llm.constants import *

logger = get_logger(__name__)


class QueryClassificationParser(BaseOutputParser[Dict[str, Any]]):
    """ÏßàÎ¨∏ Î∂ÑÎ•ò Í≤∞Í≥º ÌååÏÑú"""

    def parse(self, text: str) -> Dict[str, Any]:
        """JSON ÏùëÎãµÏùÑ ÌååÏã±ÌïòÏó¨ Î∂ÑÎ•ò Í≤∞Í≥º Î∞òÌôò"""
        try:
            # JSON Î∏îÎ°ù Ï∂îÏ∂ú
            if "```json" in text:
                json_start = text.find("```json") + 7
                json_end = text.find("```", json_start)
                json_text = text[json_start:json_end].strip()
            elif "{" in text and "}" in text:
                json_start = text.find("{")
                json_end = text.rfind("}") + 1
                json_text = text[json_start:json_end]
            else:
                json_text = text

            result = json.loads(json_text)

            # ÌïÑÏàò ÌïÑÎìú Í≤ÄÏ¶ù
            return {
                "is_legal_related": result.get("is_legal_related", False),
                "legal_category": result.get("legal_category"),
                "confidence": float(result.get("confidence", 0.0)),
                "main_topic": result.get("main_topic", ""),
                "key_entities": result.get("key_entities", []),
                "search_keywords": result.get("search_keywords", []),
                "reasoning": result.get("reasoning", ""),
            }

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Î∂ÑÎ•ò Í≤∞Í≥º ÌååÏã± Ïã§Ìå®: {str(e)}")
            # Í∏∞Î≥∏Í∞í Î∞òÌôò
            return {
                "is_legal_related": False,
                "legal_category": None,
                "confidence": 0.0,
                "main_topic": "",
                "key_entities": [],
                "search_keywords": [],
                "reasoning": "ÌååÏã± Ïã§Ìå®Î°ú Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©",
            }


class LangChainRAGService:
    """ÏôÑÏ†ÑÌïú LangChain Í∏∞Î∞ò RAG ÏÑúÎπÑÏä§ (Memory ÌÜµÌï©)"""

    def __init__(self):
        """LangChain RAG ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî"""
        self.settings = get_settings()

        # ChatOpenAI Ï¥àÍ∏∞Ìôî
        # if self.settings.OPENAI_API_KEY:
        #     self.llm = ChatOpenAI(
        #         api_key=self.settings.OPENAI_API_KEY,
        #         model=self.settings.OPENAI_MODEL,
        #         temperature=self.settings.TEMPERATURE,
        #         max_tokens=1500,
        #         streaming=False
        #     )
        #     logger.info("‚úÖ LangChain ChatOpenAI Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        if self.settings.GEMINI_API_KEY:
            self.llm = ChatGoogleGenerativeAI(
                model=self.settings.GEMINI_MODEL,
                google_api_key=self.settings.GEMINI_API_KEY,
                temperature=self.settings.TEMPERATURE,
            )
            logger.info("‚úÖ LangChain Gemini Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        else:
            self.llm = None
            logger.warning("‚ö†Ô∏è OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")

        # Í≤ÄÏÉâ ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî
        self.vector_store = VectorStore()
        self.search_service = HybridSearchService(self.vector_store)

        # Memory ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî (ÏÑ∏ÏÖòÎ≥Ñ Í¥ÄÎ¶¨)
        self._memories: Dict[str, ConversationBufferWindowMemory] = {}

        # ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï
        self.classification_system_template = self._read_prompt_file("system_prompt.txt")
        self.classification_human_template = self._read_prompt_file("human_template.txt")
        self.answer_system_template = self._read_prompt_file("answer_system_template.txt")
        self.answer_human_template = self._read_prompt_file("answer_human_template.txt")

        # ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ§Ï†ï
        self._setup_prompt_templates()

        # ÏôÑÏ†ÑÌïú RAG Ï≤¥Ïù∏ Íµ¨ÏÑ±
        self._setup_rag_chain()

        logger.info("‚úÖ LangChain RAG ÏÑúÎπÑÏä§ (Memory ÌÜµÌï©) Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")

    def _read_prompt_file(self, prompt_file_name: str):
        file_path = os.path.join(self.settings.SECRET_PATH, prompt_file_name)
        prompt_text = ""
        if os.path.isfile(file_path):
            with open(file_path, "r") as f:
                prompt_text = f.read()
        return prompt_text

    def _get_or_create_memory(self, session_id: str) -> ConversationBufferWindowMemory:
        """ÏÑ∏ÏÖòÎ≥Ñ Memory ÏÉùÏÑ± ÎòêÎäî Ï°∞Ìöå"""
        if session_id not in self._memories:
            # ÎåÄÌôî ÏúàÎèÑÏö∞ Î©îÎ™®Î¶¨ ÏÉùÏÑ± (ÏµúÍ∑º 10Í∞ú Î©îÏãúÏßÄÎßå Ïú†ÏßÄ)
            self._memories[session_id] = ConversationBufferWindowMemory(
                k=10,  # ÏµúÍ∑º 10Í∞ú Î©îÏãúÏßÄÎßå Í∏∞Ïñµ
                memory_key="chat_history",
                return_messages=True,
                human_prefix="ÏÇ¨Ïö©Ïûê",
                ai_prefix="Law Mate",
            )
            logger.debug(f"üß† ÏÉà Memory ÏÉùÏÑ±: {session_id}")

        return self._memories[session_id]

    def _setup_prompt_templates(self):
        """ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ§Ï†ï (Memory Ìè¨Ìï®)"""

        # 1. ÏßàÎ¨∏ Î∂ÑÎ•ò ÌîÑÎ°¨ÌîÑÌä∏ (ÎåÄÌôî Îß•ÎùΩ Ìè¨Ìï®)
        self.classification_prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessagePromptTemplate.from_template(self.classification_system_template),
                HumanMessagePromptTemplate.from_template(self.classification_human_template),
            ]
        )

        # 2. ÏµúÏ¢Ö ÎãµÎ≥Ä ÏÉùÏÑ± ÌîÑÎ°¨ÌîÑÌä∏ (ÎåÄÌôî Îß•ÎùΩ Ìè¨Ìï®)
        self.answer_prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessagePromptTemplate.from_template(self.answer_system_template),
                HumanMessagePromptTemplate.from_template(self.answer_human_template),
            ]
        )

    def _setup_rag_chain(self):
        """ÏôÑÏ†ÑÌïú RAG Ï≤¥Ïù∏ Íµ¨ÏÑ±"""
        if not self.llm:
            self.rag_chain = None
            return

        # ÌååÏÑú Ï¥àÍ∏∞Ìôî
        classification_parser = QueryClassificationParser()
        answer_parser = StrOutputParser()

        # 1. ÏßàÎ¨∏ Î∂ÑÎ•ò Ï≤¥Ïù∏
        classification_chain = self.classification_prompt | self.llm | classification_parser

        # 2. Î¨∏ÏÑú Í≤ÄÏÉâ Ìï®Ïàò
        async def retrieve_documents_async(classification_result: Dict[str, Any]) -> Dict[str, Any]:
            """Î∂ÑÎ•ò Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú Î¨∏ÏÑú Í≤ÄÏÉâ (ÎπÑÎèôÍ∏∞)"""
            try:
                # Í≤ÄÏÉâ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
                search_keywords = classification_result.get("search_keywords", [])
                main_topic = classification_result.get("main_topic", "")

                # Í≤ÄÏÉâ ÏøºÎ¶¨ Íµ¨ÏÑ±
                search_query = " ".join(search_keywords) if search_keywords else main_topic

                if not search_query.strip():
                    logger.warning("‚ö†Ô∏è Í≤ÄÏÉâ ÏøºÎ¶¨Í∞Ä ÎπÑÏñ¥ÏûàÏùå")
                    return {
                        "classification_result": classification_result,
                        "retrieved_docs": [],
                        "search_performed": False,
                    }

                logger.info(f"üîç Î¨∏ÏÑú Í≤ÄÏÉâ: '{search_query}'")

                # ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ ÏàòÌñâ
                retrieved_docs = await self.search_service.search(
                    query=search_query,
                    top_k=self.settings.TOP_K_DOCUMENTS,
                    similarity_threshold=self.settings.SIMILARITY_THRESHOLD,
                )

                logger.info(f"üìö Í≤ÄÏÉâ Í≤∞Í≥º: {len(retrieved_docs)}Í∞ú Î¨∏ÏÑú")

                return {
                    "classification_result": classification_result,
                    "retrieved_docs": retrieved_docs,
                    "search_performed": True,
                    "search_query": search_query,
                }

            except Exception as e:
                logger.error(f"‚ùå Î¨∏ÏÑú Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}")
                return {
                    "classification_result": classification_result,
                    "retrieved_docs": [],
                    "search_performed": False,
                    "error": str(e),
                }

        # ÎèôÍ∏∞ ÎûòÌçº Ìï®Ïàò
        def retrieve_documents(classification_result: Dict[str, Any]) -> Dict[str, Any]:
            """Î¨∏ÏÑú Í≤ÄÏÉâ ÎèôÍ∏∞ ÎûòÌçº"""
            return asyncio.create_task(retrieve_documents_async(classification_result))

        # 3. ÎãµÎ≥Ä ÏÉùÏÑ±Ïö© Îç∞Ïù¥ÌÑ∞ Ìè¨Îß∑ÌåÖ
        def format_for_answer(search_result: Dict[str, Any]) -> Dict[str, Any]:
            """ÎãµÎ≥Ä ÏÉùÏÑ±ÏùÑ ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ Ìè¨Îß∑ÌåÖ"""
            classification = search_result["classification_result"]
            retrieved_docs = search_result.get("retrieved_docs", [])

            # Î∂ÑÎ•ò Í≤∞Í≥º Ìè¨Îß∑ÌåÖ
            classification_text = f"""Î≤ïÎ•† Í¥ÄÎ†®ÏÑ±: {classification.get('is_legal_related', False)}
Î≤ïÎ•† Î∂ÑÏïº: {classification.get('legal_category', 'ÏóÜÏùå')}
Ï£ºÏöî Ï£ºÏ†ú: {classification.get('main_topic', '')}
ÌïµÏã¨ Í∞úÏ≤¥: {', '.join(classification.get('key_entities', []))}
Î∂ÑÎ•ò Í∑ºÍ±∞: {classification.get('reasoning', '')}
Ïã†Î¢∞ÎèÑ: {classification.get('confidence', 0.0):.2f}"""

            # Í≤ÄÏÉâ Î¨∏ÏÑú Ìè¨Îß∑ÌåÖ
            if retrieved_docs:
                docs_text = ""
                for i, doc in enumerate(retrieved_docs[:3], 1):
                    docs_text += f"{i}. Ï∂úÏ≤ò: {doc.get('source', 'Ïïå Ïàò ÏóÜÏùå')}\n"
                    docs_text += f"   ÎÇ¥Ïö©: {doc.get('content', '')}\n"
                    docs_text += f"   Ïú†ÏÇ¨ÎèÑ: {doc.get('similarity_score', 0.0):.2f}\n\n"
            else:
                docs_text = "Í¥ÄÎ†® Î≤ïÎ•† Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            return {
                "classification_result": classification_text.strip(),
                "retrieved_docs": docs_text.strip(),
                "original_query": search_result.get("original_query", ""),
                # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î≥¥Ï°¥
                "_classification": classification,
                "_retrieved_docs": retrieved_docs,
                "_search_performed": search_result.get("search_performed", False),
            }

        # 4. ÏµúÏ¢Ö ÎãµÎ≥Ä ÏÉùÏÑ± Ï≤¥Ïù∏
        answer_chain = self.answer_prompt | self.llm | answer_parser

        # 5. ÏôÑÏ†ÑÌïú RAG Ï≤¥Ïù∏ Íµ¨ÏÑ± (Îã®ÏàúÌôîÎêú Î≤ÑÏ†Ñ)
        self.classification_chain = classification_chain
        self.answer_chain = answer_chain
        self.retrieve_documents_func = retrieve_documents_async
        self.format_for_answer_func = format_for_answer

        logger.info("‚úÖ ÏôÑÏ†ÑÌïú LangChain RAG Ï≤¥Ïù∏ ÏÑ§Ï†ï ÏôÑÎ£å")

    async def process_query(
        self, query: str, session_id: str = "default", context_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """RAG ÌååÏù¥ÌîÑÎùºÏù∏ÏúºÎ°ú ÏßàÎ¨∏ Ï≤òÎ¶¨ (Memory ÌÜµÌï©)"""
        try:
            if not self.llm:
                return self._generate_fallback_response(query, context_info)

            logger.info(f"üöÄ LangChain RAG ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏûë: '{query[:50]}...' (ÏÑ∏ÏÖò: {session_id})")

            # ÏÑ∏ÏÖòÎ≥Ñ Memory Ï°∞Ìöå/ÏÉùÏÑ±
            memory = self._get_or_create_memory(session_id)

            # ÎåÄÌôî Í∏∞Î°ùÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
            chat_history = ""
            if hasattr(memory, "chat_memory") and memory.chat_memory.messages:
                # ÏµúÍ∑º ÎåÄÌôî Í∏∞Î°ùÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
                recent_messages = memory.chat_memory.messages[-6:]  # ÏµúÍ∑º 3ÌÑ¥
                for msg in recent_messages:
                    if hasattr(msg, "content"):
                        role = "ÏÇ¨Ïö©Ïûê" if msg.__class__.__name__ == "HumanMessage" else "Law Mate"
                        chat_history += f"{role}: {msg.content}\n"

            # ÏôÑÏ†ÑÌïú RAG Ï≤¥Ïù∏ Ïã§Ìñâ (ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï∂îÏ†Å)
            with get_openai_callback() as cb:
                # 1Îã®Í≥Ñ: ÏßàÎ¨∏ Î∂ÑÎ•ò (ÎåÄÌôî Îß•ÎùΩ Ìè¨Ìï®)
                logger.debug("1Ô∏è‚É£ ÏßàÎ¨∏ Î∂ÑÎ•ò Ï§ë... (ÎåÄÌôî Îß•ÎùΩ Ìè¨Ìï®)")
                classification_input = {"query": query, "chat_history": chat_history.strip()}
                classification_result = await self.classification_chain.ainvoke(classification_input)

                # 2Îã®Í≥Ñ: Î¨∏ÏÑú Í≤ÄÏÉâ
                logger.debug("2Ô∏è‚É£ Î¨∏ÏÑú Í≤ÄÏÉâ Ï§ë...")
                search_result = await self.retrieve_documents_func(classification_result)
                search_result["original_query"] = query

                # 3Îã®Í≥Ñ: ÎãµÎ≥Ä ÏÉùÏÑ±Ïö© Îç∞Ïù¥ÌÑ∞ Ìè¨Îß∑ÌåÖ
                logger.debug("3Ô∏è‚É£ Îç∞Ïù¥ÌÑ∞ Ìè¨Îß∑ÌåÖ Ï§ë...")
                formatted_data = self.format_for_answer_func(search_result)
                formatted_data["chat_history"] = chat_history.strip()

                # 4Îã®Í≥Ñ: ÏµúÏ¢Ö ÎãµÎ≥Ä ÏÉùÏÑ± (ÎåÄÌôî Îß•ÎùΩ Ìè¨Ìï®)
                logger.debug("4Ô∏è‚É£ ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë... (ÎåÄÌôî Îß•ÎùΩ Ìè¨Ìï®)")
                answer = await self.answer_chain.ainvoke(formatted_data)

                # 5Îã®Í≥Ñ: MemoryÏóê ÎåÄÌôî Ï†ÄÏû•
                logger.debug("5Ô∏è‚É£ ÎåÄÌôî Í∏∞Î°ù Ï†ÄÏû• Ï§ë...")
                memory.save_context({"input": query}, {"output": answer})

            # Í≤∞Í≥º Ï∂îÏ∂ú
            classification = formatted_data["_classification"]
            retrieved_docs = formatted_data["_retrieved_docs"]
            search_performed = formatted_data["_search_performed"]

            # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            confidence = self._calculate_confidence(answer, retrieved_docs, classification.get("confidence", 0.0))

            logger.info(f"‚úÖ LangChain RAG ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å (ÌÜ†ÌÅ∞: {cb.total_tokens}, ÏÑ∏ÏÖò: {session_id})")

            return {
                "answer": answer,
                "confidence": confidence,
                "classification": {
                    "is_legal_related": classification.get("is_legal_related", False),
                    "category": classification.get("legal_category"),
                    "confidence": classification.get("confidence", 0.0),
                    "reason": f"LangChain Î∂ÑÎ•ò: {classification.get('reasoning', '')}",
                    "main_topic": classification.get("main_topic", ""),
                    "key_entities": classification.get("key_entities", []),
                    "is_follow_up": classification.get("is_follow_up", False),
                },
                "retrieved_docs": retrieved_docs,
                "search_performed": search_performed,
                "tokens_used": cb.total_tokens,
                "model": self.settings.OPENAI_MODEL,
                "cost": cb.total_cost if hasattr(cb, "total_cost") else 0.0,
                "pipeline_type": "langchain_full_rag_with_memory",
                "session_id": session_id,
                "has_context": len(chat_history) > 0,
            }

        except Exception as e:
            logger.error(f"‚ùå LangChain RAG ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìå®: {str(e)}")

            # API ÌÇ§ Ïò§Î•òÏù∏ Í≤ΩÏö∞ Îçî ÎÇòÏùÄ Ìè¥Î∞± Ï†úÍ≥µ
            if "401" in str(e) or "api_key" in str(e).lower():
                return self._generate_enhanced_fallback_response(query, session_id, context_info)

            return {
                "answer": f"Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}",
                "confidence": 0.0,
                "classification": {
                    "is_legal_related": False,
                    "category": None,
                    "confidence": 0.0,
                    "reason": f"ÌååÏù¥ÌîÑÎùºÏù∏ Ïò§Î•ò: {str(e)}",
                },
                "error": str(e),
                "pipeline_type": "langchain_full_rag_with_memory",
                "session_id": session_id,
            }

    def _calculate_confidence(
        self, answer: str, retrieved_docs: List[Dict[str, Any]], classification_confidence: float
    ) -> float:
        """Ï¢ÖÌï© Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        confidence = classification_confidence * 0.4  # Î∂ÑÎ•ò Ïã†Î¢∞ÎèÑ 40%

        # ÎãµÎ≥Ä Í∏∏Ïù¥ Í∏∞Î∞ò Ï°∞Ï†ï (20%)
        if len(answer) > 100:
            confidence += 0.1
        if len(answer) > 300:
            confidence += 0.1

        # Ï∞∏Ï°∞ Î¨∏ÏÑú Ïàò Í∏∞Î∞ò Ï°∞Ï†ï (30%)
        if retrieved_docs:
            confidence += min(len(retrieved_docs) * 0.1, 0.3)

        # Î≤ïÎ•† ÌÇ§ÏõåÎìú Ìè¨Ìï® Ïó¨Î∂Ä (10%)
        keyword_count = sum(1 for keyword in LEGAL_KEYWORDS if keyword in answer)
        confidence += min(keyword_count * 0.02, 0.1)

        return min(max(confidence, 0.0), 1.0)

    def _generate_fallback_response(self, query: str, context_info: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Ìè¥Î∞± ÏùëÎãµ ÏÉùÏÑ± (API ÌÇ§Í∞Ä ÏóÜÏùÑ Îïå)"""
        return {
            "answer": f"ÏïàÎÖïÌïòÏÑ∏Ïöî! Law MateÏûÖÎãàÎã§. '{query}'Ïóê ÎåÄÌïú ÎãµÎ≥ÄÏùÑ ÏúÑÌï¥ OpenAI API ÌÇ§Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.",
            "confidence": 0.0,
            "classification": {
                "is_legal_related": False,
                "category": None,
                "confidence": 0.0,
                "reason": "API ÌÇ§ ÏóÜÏùåÏúºÎ°ú Î∂ÑÎ•ò Î∂àÍ∞Ä",
            },
            "retrieved_docs": [],
            "search_performed": False,
            "model": "fallback",
            "tokens_used": 0,
            "pipeline_type": "langchain_full_rag",
        }

    def _generate_enhanced_fallback_response(
        self, query: str, session_id: str, context_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Ìñ•ÏÉÅÎêú Ìè¥Î∞± ÏùëÎãµ ÏÉùÏÑ± (API ÌÇ§ Ïò§Î•ò Ïãú Îçî ÎÇòÏùÄ ÎãµÎ≥Ä)"""

        # Í∏∞Î≥∏Ï†ÅÏù∏ Í∑úÏπô Í∏∞Î∞ò Î∂ÑÎ•ò
        query_lower = query.lower()

        legal_category = None
        is_legal_related = False

        # Í∞ÑÎã®Ìïú ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•ò
        for category, keywords in CATEGORY_KEYWORDS.items():
            if any(keyword in query_lower for keyword in keywords):
                legal_category = category
                is_legal_related = True
                break

        # MemoryÏóê Í∏∞Î°ù (ÏÑ∏ÏÖòÏù¥ ÏûàÎäî Í≤ΩÏö∞)
        if session_id != "default":
            memory = self._get_or_create_memory(session_id)
            fallback_answer = (
                f"ÏïàÎÖïÌïòÏÑ∏Ïöî! Law MateÏûÖÎãàÎã§. '{query}'Ïóê ÎåÄÌïú ÏßàÎ¨∏ÏùÑ Î∞õÏïòÏäµÎãàÎã§. "
                f"ÌòÑÏû¨ OpenAI API Ïó∞Í≤∞Ïóê Î¨∏Ï†úÍ∞Ä ÏûàÏñ¥ ÏôÑÏ†ÑÌïú ÎãµÎ≥ÄÏùÑ ÎìúÎ¶¨Í∏∞ Ïñ¥Î†µÏäµÎãàÎã§. "
                f"API ÌÇ§Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî."
            )

            if is_legal_related:
                fallback_answer += f" ÏßàÎ¨∏Ïù¥ {legal_category} Î∂ÑÏïºÏôÄ Í¥ÄÎ†®Îêú Í≤É Í∞ôÏäµÎãàÎã§."

            # MemoryÏóê Ï†ÄÏû•
            memory.save_context({"input": query}, {"output": fallback_answer})
        else:
            fallback_answer = f"API ÌÇ§ ÏÑ§Ï†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§. '{query}' ÏßàÎ¨∏Ïóê ÎåÄÌïú ÎãµÎ≥ÄÏùÑ ÏúÑÌï¥ OpenAI API ÌÇ§Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî."

        return {
            "answer": fallback_answer,
            "confidence": 0.3 if is_legal_related else 0.1,
            "classification": {
                "is_legal_related": is_legal_related,
                "category": legal_category,
                "confidence": 0.7 if is_legal_related else 0.0,
                "reason": f"Í∑úÏπô Í∏∞Î∞ò Î∂ÑÎ•ò (API ÌÇ§ Ïò§Î•òÎ°ú Ïù∏Ìïú Ìè¥Î∞±): {legal_category or 'ÏùºÎ∞ò'}",
            },
            "retrieved_docs": [],
            "search_performed": False,
            "model": "fallback_enhanced",
            "tokens_used": 0,
            "pipeline_type": "langchain_full_rag_with_memory",
            "session_id": session_id,
            "fallback_reason": "OpenAI API ÌÇ§ Ïò§Î•ò",
        }

    def clear_memory(self, session_id: str) -> bool:
        """ÌäπÏ†ï ÏÑ∏ÏÖòÏùò Memory Ï¥àÍ∏∞Ìôî"""
        if session_id in self._memories:
            self._memories[session_id].clear()
            logger.info(f"üßπ Memory Ï¥àÍ∏∞Ìôî: {session_id}")
            return True
        return False

    def delete_memory(self, session_id: str) -> bool:
        """ÌäπÏ†ï ÏÑ∏ÏÖòÏùò Memory ÏÇ≠Ï†ú"""
        if session_id in self._memories:
            del self._memories[session_id]
            logger.info(f"üóëÔ∏è Memory ÏÇ≠Ï†ú: {session_id}")
            return True
        return False

    def get_memory_stats(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        """Memory ÌÜµÍ≥Ñ Ï†ïÎ≥¥"""
        try:
            if session_id:
                # ÌäπÏ†ï ÏÑ∏ÏÖòÏùò ÌÜµÍ≥Ñ
                memory = self._memories.get(session_id)
                if memory and hasattr(memory, "chat_memory"):
                    messages = memory.chat_memory.messages
                    return {
                        "session_id": session_id,
                        "message_count": len(messages),
                        "messages": [
                            {
                                "role": "user" if msg.__class__.__name__ == "HumanMessage" else "assistant",
                                "content": msg.content[:100] + "..." if len(msg.content) > 100 else msg.content,
                            }
                            for msg in messages
                        ],
                        "memory_type": "ConversationBufferWindowMemory",
                    }
                else:
                    return {
                        "session_id": session_id,
                        "message_count": 0,
                        "messages": [],
                        "memory_type": "ConversationBufferWindowMemory",
                    }
            else:
                # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ
                return {
                    "total_sessions": len(self._memories),
                    "memory_info": {
                        session_id: {
                            "message_count": len(memory.chat_memory.messages) if hasattr(memory, "chat_memory") else 0,
                            "memory_type": memory.__class__.__name__,
                        }
                        for session_id, memory in self._memories.items()
                    },
                }
        except Exception as e:
            logger.error(f"‚ùå Memory ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")
            return {}

    async def get_pipeline_info(self) -> Dict[str, Any]:
        """ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        return {
            "pipeline_type": "langchain_full_rag_with_memory",
            "llm_available": self.llm is not None,
            "model": self.settings.OPENAI_MODEL if self.llm else None,
            "temperature": self.settings.TEMPERATURE,
            "chain_configured": self.llm is not None,
            "memory_enabled": True,
            "memory_sessions": len(self._memories),
            "components": {
                "classification": "LangChain + LLM + Memory",
                "retrieval": "HybridSearch (BM25 + Vector)",
                "generation": "LangChain + LLM + Memory",
                "memory": "ConversationBufferWindowMemory",
            },
            "flow": "Query + Memory ‚Üí Classification ‚Üí Retrieval ‚Üí Generation ‚Üí Memory Update",
        }
