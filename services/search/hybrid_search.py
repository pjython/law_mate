"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„œë¹„ìŠ¤
BM25ì™€ ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import re
import math
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor
import asyncio

from core.config import get_settings
from core.logging.config import get_logger

logger = get_logger(__name__)


class BM25:
    """BM25 ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„"""

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.documents = []
        self.doc_lengths = []
        self.avg_doc_length = 0
        self.term_frequencies = []
        self.document_frequencies = defaultdict(int)
        self.idf_cache = {}

    def fit(self, documents: List[str]) -> None:
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.documents = documents
        self.doc_lengths = []
        self.term_frequencies = []
        self.document_frequencies = defaultdict(int)
        self.idf_cache = {}

        # ê° ë¬¸ì„œë¥¼ í† í°í™”í•˜ê³  í†µê³„ ê³„ì‚°
        for doc in documents:
            tokens = self._tokenize(doc)
            self.doc_lengths.append(len(tokens))

            # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            term_freq = Counter(tokens)
            self.term_frequencies.append(term_freq)

            # ë¬¸ì„œ ë¹ˆë„ ê³„ì‚°
            for term in set(tokens):
                self.document_frequencies[term] += 1

        # í‰ê·  ë¬¸ì„œ ê¸¸ì´ ê³„ì‚°
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths) if self.doc_lengths else 0

        # IDF ì‚¬ì „ ê³„ì‚°
        for term in self.document_frequencies:
            self.idf_cache[term] = self._calculate_idf(term)

    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í°í™”"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        text = re.sub(r"[^\w\sê°€-í£]", " ", text.lower())
        tokens = text.split()

        # í•œê¸€ ë‹¨ì–´ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
        result_tokens = []
        for token in tokens:
            if len(token) >= 2:  # 2ê¸€ì ì´ìƒë§Œ ì‚¬ìš©
                result_tokens.append(token)

                # í•œê¸€ ë‹¨ì–´ì˜ ê²½ìš° 2-gramë„ ì¶”ê°€
                if re.match(r"[ê°€-í£]+", token) and len(token) >= 3:
                    for i in range(len(token) - 1):
                        result_tokens.append(token[i : i + 2])

        return result_tokens

    def _calculate_idf(self, term: str) -> float:
        """IDF ê³„ì‚°"""
        df = self.document_frequencies.get(term, 0)
        if df == 0:
            return 0
        return math.log((len(self.documents) - df + 0.5) / (df + 0.5))

    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """BM25 ê²€ìƒ‰"""
        query_tokens = self._tokenize(query)
        scores = []

        for doc_idx in range(len(self.documents)):
            score = 0
            doc_length = self.doc_lengths[doc_idx]
            term_freq = self.term_frequencies[doc_idx]

            for term in query_tokens:
                if term in term_freq:
                    tf = term_freq[term]
                    idf = self.idf_cache.get(term, 0)

                    # BM25 ì ìˆ˜ ê³„ì‚°
                    numerator = tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length))
                    score += idf * (numerator / denominator)

            scores.append((doc_idx, score))

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]


class HybridSearchService:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""

    def __init__(self, vector_store):
        self.vector_store = vector_store
        self.settings = get_settings()
        self.bm25 = BM25()
        self.documents = []
        self.document_metadata = []
        self.bm25_index_built = False

        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.bm25_weight = self.settings.BM25_WEIGHT
        self.vector_weight = self.settings.VECTOR_WEIGHT

    async def build_indexes(self, documents: List[Dict[str, Any]]) -> None:
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            logger.info("ğŸ” ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

            if not documents:
                logger.warning("âš ï¸ ì¸ë±ìŠ¤ êµ¬ì¶•í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return

            # ë¬¸ì„œ ë°ì´í„° ì €ì¥
            self.documents = []
            self.document_metadata = []

            for doc in documents:
                content = doc.get("content", "")
                if content:
                    self.documents.append(content)
                    self.document_metadata.append(
                        {
                            "source": doc.get("source", "unknown"),
                            "metadata": doc.get("metadata", {}),
                            "original_doc": doc,
                        }
                    )

            # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
            await self._build_bm25_index()

            # ë²¡í„° ì¸ë±ìŠ¤ëŠ” vector_storeì—ì„œ ì²˜ë¦¬
            logger.info("âœ… ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            raise

    async def _build_bm25_index(self) -> None:
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            logger.debug("ğŸ”¤ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• (CPU ì§‘ì•½ì  ì‘ì—…)
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                await loop.run_in_executor(executor, self.bm25.fit, self.documents)

            self.bm25_index_built = True
            logger.debug("âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            raise

    async def search(self, query: str, top_k: int = 5, similarity_threshold: float = 0.7) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            logger.debug(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: '{query[:50]}...'")

            if not query.strip():
                logger.warning("âš ï¸ ë¹ˆ ê²€ìƒ‰ ì¿¼ë¦¬")
                return []

            # ë³‘ë ¬ë¡œ BM25ì™€ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            bm25_task = self._bm25_search(query, top_k * 2)  # ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            vector_task = self._vector_search(query, top_k * 2, similarity_threshold)

            bm25_results, vector_results = await asyncio.gather(bm25_task, vector_task)

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ë° ê²°í•©
            hybrid_results = self._combine_results(bm25_results, vector_results, top_k)

            logger.debug(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(hybrid_results)}ê°œ ê²°ê³¼")
            return hybrid_results

        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _bm25_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """BM25 ê²€ìƒ‰"""
        try:
            if not self.bm25_index_built or not self.documents:
                logger.warning("âš ï¸ BM25 ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []

            logger.debug(f"ğŸ”¤ BM25 ê²€ìƒ‰ ìˆ˜í–‰: '{query[:30]}...'")

            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ BM25 ê²€ìƒ‰ ìˆ˜í–‰
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                bm25_scores = await loop.run_in_executor(executor, self.bm25.search, query, top_k)

            # ê²°ê³¼ ë³€í™˜
            results = []
            for doc_idx, score in bm25_scores:
                if score > 0:  # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ
                    results.append(
                        {
                            "content": self.documents[doc_idx],
                            "source": self.document_metadata[doc_idx]["source"],
                            "metadata": self.document_metadata[doc_idx]["metadata"],
                            "bm25_score": score,
                            "doc_index": doc_idx,
                        }
                    )

            logger.debug(f"âœ… BM25 ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results

        except Exception as e:
            logger.error(f"âŒ BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _vector_search(self, query: str, top_k: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰"""
        try:
            logger.debug(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰: '{query[:30]}...'")

            # ë²¡í„° ìŠ¤í† ì–´ë¥¼ í†µí•œ ê²€ìƒ‰
            vector_results = await self.vector_store.search_similar(query, top_k, similarity_threshold)

            # ê²°ê³¼ì— ë²¡í„° ì ìˆ˜ ì¶”ê°€
            for result in vector_results:
                result["vector_score"] = result.get("similarity_score", 0.0)

            logger.debug(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(vector_results)}ê°œ ê²°ê³¼")
            return vector_results

        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def _combine_results(
        self, bm25_results: List[Dict[str, Any]], vector_results: List[Dict[str, Any]], top_k: int
    ) -> List[Dict[str, Any]]:
        """BM25ì™€ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ê²°í•©"""
        try:
            logger.debug("ğŸ”— ê²€ìƒ‰ ê²°ê³¼ ê²°í•© ì¤‘...")

            # ë¬¸ì„œë³„ ì ìˆ˜ ì§‘ê³„
            doc_scores = defaultdict(
                lambda: {
                    "bm25_score": 0.0,
                    "vector_score": 0.0,
                    "hybrid_score": 0.0,
                    "content": "",
                    "source": "",
                    "metadata": {},
                    "found_in": [],
                }
            )

            # BM25 ê²°ê³¼ ì²˜ë¦¬
            max_bm25_score = max([r.get("bm25_score", 0) for r in bm25_results]) if bm25_results else 1.0
            for result in bm25_results:
                content = result["content"]
                normalized_bm25 = result.get("bm25_score", 0) / max_bm25_score if max_bm25_score > 0 else 0

                doc_scores[content]["bm25_score"] = normalized_bm25
                doc_scores[content]["content"] = content
                doc_scores[content]["source"] = result.get("source", "")
                doc_scores[content]["metadata"] = result.get("metadata", {})
                doc_scores[content]["found_in"].append("bm25")

            # ë²¡í„° ê²°ê³¼ ì²˜ë¦¬
            for result in vector_results:
                content = result["content"]
                vector_score = result.get("vector_score", 0)

                doc_scores[content]["vector_score"] = vector_score
                if not doc_scores[content]["content"]:  # BM25ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°
                    doc_scores[content]["content"] = content
                    doc_scores[content]["source"] = result.get("source", "")
                    doc_scores[content]["metadata"] = result.get("metadata", {})
                doc_scores[content]["found_in"].append("vector")

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            hybrid_results = []
            for content, scores in doc_scores.items():
                if content:  # ë¹ˆ ë¬¸ì„œ ì œì™¸
                    hybrid_score = scores["bm25_score"] * self.bm25_weight + scores["vector_score"] * self.vector_weight

                    # ë‘ ê²€ìƒ‰ì—ì„œ ëª¨ë‘ ë°œê²¬ëœ ê²½ìš° ë³´ë„ˆìŠ¤
                    if len(scores["found_in"]) > 1:
                        hybrid_score *= 1.1

                    hybrid_results.append(
                        {
                            "content": content,
                            "source": scores["source"],
                            "metadata": scores["metadata"],
                            "hybrid_score": hybrid_score,
                            "bm25_score": scores["bm25_score"],
                            "vector_score": scores["vector_score"],
                            "found_in": scores["found_in"],
                            "search_method": "hybrid",
                        }
                    )

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
            hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)

            # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
            final_results = hybrid_results[:top_k]

            logger.debug(f"âœ… ê²€ìƒ‰ ê²°ê³¼ ê²°í•© ì™„ë£Œ: {len(final_results)}ê°œ ìµœì¢… ê²°ê³¼")
            return final_results

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ê²°í•© ì‹¤íŒ¨: {str(e)}")
            return []

    async def get_search_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ì •ë³´"""
        try:
            stats = {
                "bm25_index_built": self.bm25_index_built,
                "total_documents": len(self.documents),
                "bm25_weight": self.bm25_weight,
                "vector_weight": self.vector_weight,
                "avg_doc_length": self.bm25.avg_doc_length if self.bm25_index_built else 0,
            }

            # ë²¡í„° ìŠ¤í† ì–´ í†µê³„ ì¶”ê°€
            if hasattr(self.vector_store, "get_collection_stats"):
                vector_stats = await self.vector_store.get_collection_stats()
                stats["vector_store"] = vector_stats

            return stats

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}

    async def health_check(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìƒíƒœ í™•ì¸"""
        try:
            health_info = {
                "status": "healthy",
                "bm25_ready": self.bm25_index_built,
                "vector_store_ready": self.vector_store is not None,
                "document_count": len(self.documents),
            }

            # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            try:
                test_results = await self.search("í…ŒìŠ¤íŠ¸", top_k=1)
                health_info["search_functional"] = True
                health_info["test_results_count"] = len(test_results)
            except Exception as e:
                health_info["search_functional"] = False
                health_info["search_error"] = str(e)

            return health_info

        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}

    async def rebuild_index(self) -> None:
        """
        ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        VectorStoreì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ BM25 ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.
        """
        try:
            logger.info("ğŸ”„ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘...")

            # ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            self.documents = []
            self.document_metadata = []
            self.bm25_index_built = False
            logger.debug("ğŸ§¹ ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

            # VectorStoreì—ì„œ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
            logger.debug("ğŸ“„ VectorStoreì—ì„œ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
            all_documents = await self.vector_store.get_all_documents()

            if not all_documents:
                logger.warning("âš ï¸ ì¬êµ¬ì¶•í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return

            logger.info(f"ğŸ“‹ {len(all_documents)}ê°œ ë¬¸ì„œë¡œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì¤‘...")

            # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ êµ¬ì¶•
            await self.build_indexes(all_documents)

            logger.info("âœ… ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            raise

    async def rebuild_index_by_source(self, source: str) -> None:
        """
        íŠ¹ì • ì†ŒìŠ¤ì˜ ë¬¸ì„œë§Œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        íŠ¹ì • íŒŒì¼ì˜ ë¬¸ì„œë§Œ ì—…ë°ì´íŠ¸í•  ë•Œ ì‚¬ìš©
        """
        try:
            logger.info(f"ğŸ”„ ì†ŒìŠ¤ë³„ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘: {source}")

            # í•´ë‹¹ ì†ŒìŠ¤ì˜ ë¬¸ì„œ ì¡°íšŒ
            source_documents = await self.vector_store.get_documents_by_source(source)

            if not source_documents:
                logger.warning(f"âš ï¸ ì†ŒìŠ¤ '{source}'ì˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return

            # ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ í•´ë‹¹ ì†ŒìŠ¤ ë¬¸ì„œ ì œê±°
            self._remove_documents_by_source(source)

            # ìƒˆë¡œìš´ ì†ŒìŠ¤ ë¬¸ì„œë“¤ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€
            await self._add_documents_to_index(source_documents)

            logger.info(f"âœ… ì†ŒìŠ¤ë³„ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ: {source} ({len(source_documents)}ê°œ ë¬¸ì„œ)")

        except Exception as e:
            logger.error(f"âŒ ì†ŒìŠ¤ë³„ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            raise

    def _remove_documents_by_source(self, source: str) -> None:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ íŠ¹ì • ì†ŒìŠ¤ì˜ ë¬¸ì„œ ì œê±°"""
        try:
            logger.debug(f"ğŸ—‘ï¸ ì¸ë±ìŠ¤ì—ì„œ ì†ŒìŠ¤ '{source}' ë¬¸ì„œ ì œê±° ì¤‘...")

            # ì œê±°í•  ì¸ë±ìŠ¤ ì°¾ê¸°
            indices_to_remove = []
            for i, metadata in enumerate(self.document_metadata):
                if metadata.get("source") == source:
                    indices_to_remove.append(i)

            # ì—­ìˆœìœ¼ë¡œ ì œê±° (ì¸ë±ìŠ¤ ë³€ê²½ ë°©ì§€)
            for i in reversed(indices_to_remove):
                if i < len(self.documents):
                    del self.documents[i]
                if i < len(self.document_metadata):
                    del self.document_metadata[i]

            logger.debug(f"âœ… {len(indices_to_remove)}ê°œ ë¬¸ì„œ ì œê±° ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ì†ŒìŠ¤ë³„ ë¬¸ì„œ ì œê±° ì‹¤íŒ¨: {str(e)}")

    async def _add_documents_to_index(self, documents: List[Dict[str, Any]]) -> None:
        """ìƒˆë¡œìš´ ë¬¸ì„œë“¤ì„ ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€"""
        try:
            logger.debug(f"â• ì¸ë±ìŠ¤ì— {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì¤‘...")

            # ë¬¸ì„œ ë°ì´í„° ì €ì¥
            new_documents = []
            new_metadata = []

            for doc in documents:
                content = doc.get("content", "")
                if content:
                    new_documents.append(content)
                    new_metadata.append(
                        {
                            "source": doc.get("source", "unknown"),
                            "metadata": doc.get("metadata", {}),
                            "original_doc": doc,
                        }
                    )

            # ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€
            self.documents.extend(new_documents)
            self.document_metadata.extend(new_metadata)

            # BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ì „ì²´ ë¬¸ì„œë¡œ)
            if self.documents:
                await self._build_bm25_index()

            logger.debug(f"âœ… {len(new_documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            raise

    async def get_index_statistics(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        try:
            # ê¸°ë³¸ í†µê³„
            stats = await self.get_search_statistics()

            # ì†ŒìŠ¤ë³„ ë¬¸ì„œ ê°œìˆ˜ ì¶”ê°€
            source_counts = await self.vector_store.get_documents_count_by_source()
            stats["documents_by_source"] = source_counts

            # ì¸ë±ìŠ¤ ìƒíƒœ ì •ë³´
            stats["index_status"] = {
                "bm25_built": self.bm25_index_built,
                "documents_in_memory": len(self.documents),
                "metadata_count": len(self.document_metadata),
                "vector_store_count": self.vector_store.get_document_count(),
            }

            return stats

        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
